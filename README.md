# Speech and Non-Speech Audio Recognizer

A comprehensive, modular Python-based audio analysis pipeline that separates, transcribes, and classifies audio content using state-of-the-art machine learning models.

## üìã Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Components](#components)
- [Performance](#performance)
- [Future Enhancements](#future-enhancements)
- [Credits](#credits)

## üéØ Overview

The Speech and Non-Speech Audio Recognizer is designed to process audio files and generate structured analysis reports by:

1. **Separating** audio into isolated components (speech and background)
2. **Transcribing** spoken content into text with multilingual support
3. **Classifying** non-speech sounds and environmental audio
4. **Compiling** all findings into a unified, human-readable report

This pipeline excels in scenarios like podcast analysis, surveillance audio review, content moderation, and audio forensics.

### Key Benefits

- **Modularity**: Each processing step is isolated for easy debugging and extension
- **Efficiency**: Models are loaded once; processing runs on CPU with GPU acceleration support
- **Flexibility**: Configurable file paths and parameters for different input audio files
- **Accuracy**: Uses cutting-edge models achieving <10% WER for transcription and >8dB SDR for separation

## ‚ú® Key Features

‚úÖ **Source Separation**: Isolate vocals from background using Hybrid Transformer Demucs  
‚úÖ **Noise Reduction**: Spectral gating-based denoising for cleaner transcription  
‚úÖ **Multilingual Speech Recognition**: Support for 99+ languages, including Hindi and English  
‚úÖ **Sound Classification**: Identify 521+ environmental sound events using YAMNet  
‚úÖ **Structured Reports**: Clean TXT format output with complete analysis results  

## üèóÔ∏è System Architecture

The pipeline follows a linear, sequential workflow orchestrated by `main.py`:

\`\`\`
Input Audio File (.wav)
    ‚Üì
[Step 1: Separation & Denoising] ‚Üí separated/vocals.wav, separated/other.wav
    ‚Üì
[Step 2: Speech Transcription] ‚Üí transcription.txt
    ‚Üì
[Step 3: Non-Speech Analysis] ‚Üí nonspeech_report.txt
    ‚Üì
[Step 4: Report Compilation] ‚Üí final_report.txt
\`\`\`

### Architecture Details

| Step | Script | Purpose | Primary Model |
|------|--------|---------|---------------|
| 1 | `Seperator.py` | Audio source separation & denoising | Demucs (htdemucs) |
| 2 | `speech_analyser.py` | Speech-to-text transcription | OpenAI Whisper (large-v3) |
| 3 | `non_speech_analyser.py` | Sound event classification | YAMNet (TensorFlow Hub) |
| 4 | `main.py` | Orchestration & report generation | - |

## üì¶ Installation

### Prerequisites

- Python 3.8 or higher
- PyTorch (CPU or CUDA-enabled)

### Dependencies

Install all required packages via pip:

\`\`\`bash
pip install torch torchaudio demucs noisereduce openai-whisper tensorflow tensorflow-hub librosa pandas numpy
\`\`\`

### Full Installation

\`\`\`bash
# Clone the repository
git clone <repository-url>
cd speech-nonspeech-recognizer

# Install dependencies
pip install -r requirements.txt
\`\`\`

## üöÄ Usage

### Basic Workflow

1. Place your input audio file (WAV format, 16kHz recommended) in the project directory
2. Update the `INPUT_AUDIO` path in `Seperator.py`
3. Run the pipeline:

\`\`\`bash
python main.py
\`\`\`

### Output

The pipeline generates:
- **Folder**: `separated/` - Contains isolated audio tracks (vocals.wav, other.wav, etc.)
- **File**: `Speech text output/transcription.txt` - Transcribed speech content
- **File**: `Speech text output/nonspeech_report.txt` - Classified sounds with confidence scores
- **File**: `Final report/final_report.txt` - Complete unified analysis report

### Example Input/Output

**Input**: 30-second podcast snippet with speech, background music, and traffic noise

**Output Report**:
\`\`\`
=== SPEECH AND NON-SPEECH ANALYSIS REPORT ===
Generated: 2024-01-15 14:32:00

--- TRANSCRIBED SPEECH ---
"Welcome to our podcast where we discuss recent developments..."

--- DETECTED NON-SPEECH SOUNDS ---
Background Music: 0.912
Traffic Noise: 0.687
Door Slam: 0.423
\`\`\`

## üîß Components

### Step 1: Audio Source Separation & Denoising (`Seperator.py`)

**Purpose**: Decompose audio into isolated stems (vocals vs. background/instruments)

**Models Used**:
- **Demucs (htdemucs)**: State-of-the-art neural source separation combining convolutional and transformer layers
- **noisereduce**: Spectral gating-based noise reduction with Wiener filtering

**Key Features**:
- Separates audio into 4 sources: vocals, drums, bass, other
- Aggressive noise reduction on vocal track (`prop_decrease=0.9`)
- Handles non-stationary noise like echoes

**Inputs**: Raw audio file (WAV, 16kHz)  
**Outputs**: Separated stems in `separated/` folder

\`\`\`python
# Model Loading
model = get_model("htdemucs")
model.cpu().eval()

# Separation & Denoising
sources = apply.apply_model(model, wav, device="cpu")[0]
reduced = nr.reduce_noise(y=audio_np, sr=sr, prop_decrease=0.9, stationary=False)
\`\`\`

### Step 2: Speech-to-Text Transcription (`speech_analyser.py`)

**Purpose**: Transcribe isolated vocal track with multilingual support

**Models Used**:
- **OpenAI Whisper (large-v3)**: Transformer encoder-decoder for automatic speech recognition
- Trained on 680k+ hours of multilingual data
- Supports 99+ languages with <5% WER on benchmarks

**Key Features**:
- Automatic language detection (base model on first 30 seconds)
- Routes to appropriate language-specific model (English or Hindi)
- Beam search decoding for better accuracy
- Clean text output (no timestamps)

**Inputs**: `separated/vocals.wav` (post-separation)  
**Outputs**: `Speech text output/transcription.txt`

\`\`\`python
# Language Detection & Transcription
detect = lang_model.transcribe(path)
detected_lang = detect["language"]

if detected_lang in ["hi", "ur", "hr"]:
    result = model_hi.transcribe(path, language="hi")
else:
    result = model_en.transcribe(path, language="en")
\`\`\`

### Step 3: Non-Speech Sound Classification (`non_speech_analyser.py`)

**Purpose**: Classify environmental sounds and background audio

**Models Used**:
- **YAMNet**: Google's pre-trained audio event classification model
- MobileNetV1-based CNN trained on AudioSet (2M+ clips)
- 521 sound event classes with real-time inference capability

**Key Features**:
- Lightweight (1MB model size)
- Mean pooling aggregation for overall audio fingerprint
- Automatic filtering of speech-related labels
- Top 10 non-speech sound classes with confidence scores

**Inputs**: `separated/other.wav`  
**Outputs**: `Speech text output/nonspeech_report.txt`

\`\`\`python
# Model Loading & Inference
yamnet = hub.load("https://tfhub.dev/google/yamnet/1")
scores, _, _ = yamnet(waveform)  # [frames, 521]

# Filtering & Aggregation
avg_scores = np.mean(scores.numpy(), axis=0)
speech_terms = ["Speech", "Conversation", "Narration", "Babbling"]
top_indices = np.argsort(avg_scores)[::-1]
\`\`\`

### Step 4: Report Compilation (`main.py`)

**Purpose**: Orchestrate the pipeline and generate unified final report

**Functions**:
- Sequential script execution via subprocess
- Speech text extraction and cleaning (regex-based)
- Non-speech report parsing and aggregation
- Template-based TXT report generation
- Error handling and validation

## üìä Performance

### Processing Speed

| Audio Duration | CPU Time | GPU Time* |
|---|---|---|
| 30 seconds | ~10-20s | ~3-5s |
| 1 minute | ~20-40s | ~5-10s |
| 5 minutes | ~1-2 min | ~15-30s |

*GPU acceleration (CUDA) varies by hardware; defaults to CPU-only

### Accuracy Benchmarks

| Component | Metric | Score |
|---|---|---|
| **Separation** | Signal-to-Distortion Ratio (SDR) | ~9 dB |
| **Transcription** | Word Error Rate (WER) | <10% |
| **Classification** | Mean Average Precision (mAP) | ~0.30 |

### Input Requirements

- **Format**: WAV or convertible audio format
- **Sample Rate**: 16 kHz (auto-resampled if different)
- **Supported Languages**: 99+ (optimized for English and Hindi)

## üîÆ Future Enhancements

- **GPU Acceleration**: Full CUDA support for faster processing on large files
- **Timestamped Transcription**: Include timing information for speech segments
- **Speaker Diarization**: Identify and separate multiple speakers
- **Visualization**: Generate spectrograms and audio waveform plots
- **Batch Processing**: Process multiple files in parallel
- **Real-time Processing**: Stream-based audio processing for live inputs
- **Enhanced Filtering**: Context-aware non-speech classification with temporal analysis
- **Custom Model Integration**: Support for user-trained models

## üôè Credits

**Project Team Lead**: Harshita

This project leverages the following state-of-the-art models and libraries:
- **Demucs** (Facebook AI Research) - Audio source separation
- **OpenAI Whisper** - Automatic speech recognition
- **YAMNet** (Google) - Audio event classification
- **TensorFlow & PyTorch** - Deep learning frameworks
- **librosa** - Audio processing
